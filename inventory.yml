tuana9a-dev2:
  hosts:
    192.168.56.209:
      ansible_user: u
      ansible_python_interpreter: /home/u/github.com/tuana9a/platform/.venv/bin/python
      authorized_keys:
        - key: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEu+yuCFcAO331i+S6902+7WJHmnhponellNnhk45QF2"
          comment: "tuana9a@tuana9a-mac-air-m2.local"
nfs:
  hosts:
    192.168.56.7:
      ansible_user: u
      HOST_NAME: nfs
      parted:
        - dev_path: /dev/vdb
          mount_path: /exports/vdb/
          enabled: true
        - dev_path: /dev/vdc
          mount_path: /exports/vdc/
          enabled: false
      NFS_DIR: "/exports/nfs-client"
      nfs_server_exports:
        - path: /exports/vdb/
          config: /exports/vdb/ 192.168.56.0/24(rw,sync,no_root_squash,no_subtree_check)
        - path: /exports/vdc/
          config: /exports/vdc/ 192.168.56.0/24(rw,sync,no_root_squash,no_subtree_check)
          state: absent
      backup_nfs_items:
        - name: coder
          path: "/exports/nfs-client/coder-home-*"
          excludes:
            - .git
            - .venv
            - .m2
            - .vscode-server
            - node_modules
            - .cache
            - .terraform
            - .tfenv
            - .angular
            - .nvm
            - .npm
            - .local
            - .oh-my-zsh
            - go/pkg
            - "*.lock"
            - "*tmp*"
            - "Python-*"
            - "amazon-corretto-*"
neomorph:
  hosts:
    192.168.56.112:
      ansible_user: u
      ufw_state: "enabled"
      ufw_rules: &proxmox_ufw_rules
        - name: "Allow SSH (Limit)"
          rule: limit
          port: "22" # SSH
          proto: tcp
        - name: "Allow Proxmox Web UI"
          rule: allow
          port: "8006" # Proxmox Web
          proto: tcp
        - name: "Allow node-exporter"
          rule: allow
          port: "9100" # node-exporter
          proto: tcp
          from_ip: "192.168.56.0/24"
engineer:
  hosts:
    192.168.56.113:
      ansible_user: u
      ufw_state: "disabled"
      ufw_rules: *proxmox_ufw_rules
k8s_cluster:
  hosts:
    192.168.56.22: &i-122
      ansible_user: u
      nodename: i-122
      vmid: 122
      is_control_plane: true
      kube_pki: &kube_pki
        - /etc/kubernetes/pki/ca.crt
        - /etc/kubernetes/pki/ca.key
        - /etc/kubernetes/pki/sa.key
        - /etc/kubernetes/pki/sa.pub
        - /etc/kubernetes/pki/front-proxy-ca.crt
        - /etc/kubernetes/pki/front-proxy-ca.key
        - /etc/kubernetes/pki/etcd/ca.crt
        - /etc/kubernetes/pki/etcd/ca.key
      kubevip: &kubevip
        inf: eth0
        vip: 192.168.56.21
      authorized_keys: &k8s_control_plane_authorized_keys
        - key: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIN6bF/SOzb1XD4qo0LaZ5PVa1sCijDyQS/8oHZe9x6R5"
          comment: "ci"
          user: root
    192.168.56.23: &i-123
      ansible_user: u
      nodename: i-123
      vmid: 123
      is_control_plane: true
      kube_pki: *kube_pki
      kubevip: *kubevip
      goodbye: false
      authorized_keys: *k8s_control_plane_authorized_keys
    192.168.56.24: &i-124
      ansible_user: u
      nodename: i-124
      vmid: 124
      is_control_plane: true
      kube_pki: *kube_pki
      kubevip: *kubevip
      authorized_keys: *k8s_control_plane_authorized_keys
    192.168.56.25: &i-125
      ansible_user: u
      nodename: i-125
      vmid: 125
      is_control_plane: true
      kube_pki: *kube_pki
      kubevip: *kubevip
      goodbye: true
      authorized_keys: *k8s_control_plane_authorized_keys
    192.168.56.31: &i-131
      ansible_user: u
      nodename: i-131
      vmid: 131
      is_control_plane: false
      wireguard_clients:
        - file: k8s-cobi-5.conf
          name: k8s-cobi-5
          enabled: false
          state: stopped
        - file: i-131.conf
          name: i-131
      goodbye: false
    192.168.56.32: &i-132
      ansible_user: u
      nodename: i-132
      vmid: 132
      is_control_plane: false
      wireguard_clients:
        - file: k8s-cobi-7.conf
          name: k8s-cobi-7
          enabled: false
          state: stopped
        - file: i-132.conf
          name: i-132
    192.168.56.33: &i-133
      ansible_user: u
      nodename: i-133
      vmid: 133
      is_control_plane: false
      wireguard_clients:
        - file: i-133.conf
          name: i-133
      goodbye: false
    192.168.56.34: &i-134
      ansible_user: u
      nodename: i-134
      vmid: 134
      is_control_plane: false
      wireguard_clients:
        - file: k8s-cobi-6.conf
          name: k8s-cobi-6
          enabled: false
          state: stopped
        - file: i-134.conf
          name: i-134
k8s_cluster_operations:
  hosts:
    192.168.56.22:
      <<: *i-122
      goodbye_nodes:
        # - name: i-123
        #   is_control_plane: true
        # - name: i-125
        #   is_control_plane: true
        # - name: i-131
        # - name: i-133
orisis:
  hosts:
    orisis.tuana9a.com:
      ansible_user: root
      HOST_NAME: orisis
      haproxy_stats_enabled: false
      upstream_servers:
        - hostname: k8s-cobi-5
          ip: 10.5.115.25
          http_port: 30080
          https_port: 30443
        - hostname: k8s-cobi-6
          ip: 10.5.115.26
          http_port: 30080
          https_port: 30443
        - hostname: k8s-cobi-7
          ip: 10.5.115.27
          http_port: 30080
          https_port: 30443
        - hostname: i-131
          ip: 10.5.115.31
          http_port: 30080
          https_port: 30443
        - hostname: i-132
          ip: 10.5.115.32
          http_port: 30080
          https_port: 30443
        - hostname: i-133
          ip: 10.5.115.33
          http_port: 30080
          https_port: 30443
        - hostname: i-134
          ip: 10.5.115.34
          http_port: 30080
          https_port: 30443
